#' @title calculate_AUC_base
#' @description the base function of calculate_AUCs: build the model and calculate the AUCs
#'
#' @param n01_all size of all data labeled as class 0/1. For example, n01_all=c(800,800) represents that the size of all data labeled as class 0 is 800 and the size of all data labeled as class 1 is also 800.
#' @param n01_p size of pilot data labeled as class 0/1. For example, n01_p=c(15,15) represents that the size of pilot data labeled as class 0 is 15 and the size of pilot data labeled as class 1 is also 15.
#' @param n_train_sets size sets of training data labeled as class 0/1. For example, n_train_sets=c(c(30,30),c(90,90),c(150,150)) represents that we try 3 different sets of training data and the training size of the first set is c(30,30).
#' @param n01_test number of test data labeled as class 0/1. size of all data labeled as class 0/1. For example, n01_test=c(300,300) represents that the size of test data labeled as class 0 is 300 and the size of test data labeled as class 1 is also 300.
#' @param seed the seed you want to use to run simulations.
#' @param method
#' Choose the method you want to use: "pca2_mvnorm" , "gaussian_copula" or "smote"(\code{\link[smotefamily]{SMOTE}}).
#' The default value is "pca2_mvnorm".
#' @param model base classification model.
#' \itemize{
#' \item logistic: Logistic regression. \link{glm} function with family = 'binomial'
#' \item penlog: Penalized logistic regression with LASSO penalty. \code{\link[glmnet]{glmnet}} in \code{glmnet} package
#' \item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
#' \item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
#' \item lda: Linear Discriminant Analysis. \code{\link[MASS]{lda}} in \code{MASS} package
#' \item slda: Sparse Linear Discriminant Analysis with LASSO penalty.
#' \item nb: Naive Bayes. \code{\link[e1071]{naiveBayes}} in \code{e1071} package
#' \item nnb: Nonparametric Naive Bayes. \code{\link[naivebayes]{naive_bayes}} in \code{naivebayes} package
#' \item ada: Ada-Boost. \code{\link[ada]{ada}} in \code{ada} package
#' \item xgboost: XGBboost. \code{\link[xgboost]{xgboost}} in \code{xgboost} package
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' \item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
#' }
#'
#' @param func If you set "model" to "self", you have to pass your self-defined model function. This function should be able to take "x_train" and "y_train" as the first two inputs to train the model and then take "x_test" as the third input and return the predicted scores of x_test data. For example, \cr\cr
#' \code{library(e1071)\cr\cr
#' predict_model <- function(x_train, y_train, x_test){ \cr
#' data_trainxy<-data.frame(x_train,y_train=as.factor(y_train)) \cr
#' fit_svm<-svm(y_train~.,data=data_trainxy,probability=TRUE) \cr
#' pred_svm <- predict(fit_svm, x_test, probability=TRUE,decision.values = TRUE) \cr
#' p_svm=as.data.frame(attr(pred_svm, "probabilities"))$"1" \cr
#' return(p_svm) \cr
#' }\cr \cr
#' AUC = calculate_AUC_base(n01_all= c(800,800), n01_p=c(15,15), n01_test=c(300,300), n_train_sets = c(c(15,15),c(30,30),c(60,60),c(120,120),c(150,150)), seed=1, model=c("self","randomforest"),func=predict_model)}
#'
#' @param data_generation a parameter list that you can tell the function about the distribution and parameters you want to use to generate the data.
#' \itemize{
#' \item "gaussian" represent multivariate gaussian distribution. see \code{\link[MASS]{mvrnorm}} in \code{MASS} package. For example, data_generation=list(dist="gaussian",sigma=list(class_0=diag(5),class_1=diag(5)),mu=c(rep(0,5),rep(2,5)))
#' \item "t-distribution" represent multivariate t distribution. see \code{\link[mvtnorm]{rmvt}} in \code{mvtnorm} package. For example, data_generation=list(dist="t-distribution",sigma=list(class_0=diag(5),class_1=diag(5)),df=c(10,10),delta=c(rep(0,5),rep(2,5))).
#' }
#'
#' @param data_input Its default value is NULL and the function will use the "data_generation" parameter to generate the data. If "data_input" is not NULL, the function will ignore the "data_generation" parameter and "n01_all" parameter, and use the "data_input" as the data.
#' Your "data_input" should be a list with "x_data" matrix and "y_data" matrix. For example, \cr\cr
#' \code{
#'
#'yeast_data <- read.table("./yeast.data") \cr}
#'###\link{https://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set}\cr\cr
#'\code{
#'x_data = yeast_data[,c(2:5,8:9)]\cr
#'y_label = yeast_data[,10]\cr
#'id0 = which(y_label=="CYT" | y_label=="MIT")\cr
#'y_data = rep(1,length(y_label))\cr
#'y_data[id0] = 0\cr
#' \cr
#'data = list(x_data=x_data, y_data=y_data)\cr\cr
#'
#'AUC = calculate_AUC_base(n01_p=c(15,15), n_train_sets = c(c(15,15),c(30,30),c(60,60),c(120,120),c(150,150)), n01_test=c(300,300), seed=1, method="pca2_mvnorm", model=c("svm","randomforest"), data_input=data)\cr
#' }
#'
#' @param true_data_to_compare 0/1 variable, whether to have the true data to compare with generated data (which could make the output matrix double): if input_data==NULL(run the simulations), we will generate the true data automatically and calculate the AUCs; if you give us the input_data, please make sure that you give us sufficient input data.
#'
#' @return Return the AUCs you want to calculate
#' @export
#'
#' @examples AUC = calculate_AUC_base(n01_all= c(800,800), n01_p=c(15,15), n01_test=c(300,300), n_train_sets = c(c(15,15),c(30,30),c(60,60),c(120,120),c(150,150)), seed=1, model=c("svm","randomforest"))
calculate_AUC_base <- function(n01_all= c(800,800), n01_p=c(15,15), n_train_sets = c(c(15,15),c(30,30),c(60,60),c(120,120),c(150,150)), n01_test=c(300,300), seed=1, method="pca2_mvnorm", model=c("svm","randomforest"),func=NULL,data_generation=list(dist="t-distribution",sigma=list(class_0=diag(5),class_1=diag(5)),df=c(10,10),delta=c(rep(0,5),rep(2,5))),data_input=NULL,true_data_to_compare=1)
{
  library(PRROC)
  n0_p <- n01_p[1]
  n1_p <- n01_p[2]
  n_p=n0_p+n1_p

  n0_test <- n01_test[1]
  n1_test <- n01_test[2]
  n_test = n0_test + n1_test

  if(is.null(data_input)){
    data = generate_data(seed=seed, n01_all=n01_all, data_generation=data_generation)
  }else{
    data = data_input
    if(true_data_to_compare==1){
      if(n01_p[1]+n_train_sets[length(n_train_sets)-1]+n01_test[1] > sum(data$y_data==0)){
        stop('error: there are not sufficient class 0 input data to run the simulations! Please adjust the parameters.')
      }
      if(n01_p[2]+n_train_sets[length(n_train_sets)]+n01_test[2] > sum(data$y_data==1)){
        stop('error: there are not sufficient class 1 input data to run the simulations! Please adjust the parameters.')
      }
    }
  }

  pilot_rest_data = split_data(data$x_data, data$y_data, n_train=n_p, seed=seed)

  test_y=c(rep(0,n0_test),rep(1,n1_test))

  Loop=100

  num_of_model = length(model)

  number_of_train_sets = length(n_train_sets)/2

  dim(n_train_sets) = c(2,number_of_train_sets)

  if(true_data_to_compare==0){
    auc = array(0,dim=c(length(n_train_sets)/2,Loop,num_of_model))
  }else{
    auc = array(0,dim=c(length(n_train_sets),Loop,num_of_model))
  }

  count = 1
  for (test_from_true in c(0,1)){
    for (i in 1:number_of_train_sets){
      n0_train <- n_train_sets[1,i]
      n1_train <- n_train_sets[2,i]
      n_train = n0_train + n1_train

      p <- array(0,dim=c(Loop,n_test,num_of_model))
      # matrix(0,Loop,n_test)
      for (L in 1:Loop)
      {
        if(test_from_true==0){
          if(method == "pca2_mvnorm"){result = pilot_tfe_mvnorm_pca2(pilot_rest_data$x_train,pilot_rest_data$y_train,n0_train,n1_train,n0_test,n1_test,method=model,func=func)}
          if(method == "gaussian_copula"){result = pilot_tfe_gaussian_copula(pilot_rest_data$x_train,pilot_rest_data$y_train,n0_train,n1_train,n0_test,n1_test,method=model,func=func)}
          if(method == "smote"){result = pilot_tfe_smote(pilot_rest_data$x_train,pilot_rest_data$y_train,n0_train,n1_train,n0_test,n1_test,method=model,func=func)}

        }else if(test_from_true==1){

          train_test_data = split_data(data$x_data, data$y_data, n_train=n_train, n_test=n_test)

          if(method == "pca2_mvnorm"){result = pilot_train_pca2(train_test_data$x_train,train_test_data$y_train,train_test_data$x_test, method=model,func=func)}
          if(method == "gaussian_copula"){result = pilot_train(train_test_data$x_train,train_test_data$y_train,train_test_data$x_test, method=model,func=func)}
          if(method == "smote"){result = pilot_train(train_test_data$x_train,train_test_data$y_train,train_test_data$x_test, method=model,func=func)}
        }

        for(j in 1:num_of_model){
          p[L,,j] = result[j,]
        }

        cat(count,L,"\n")

      }


      for (k in 1: Loop)
      {
        index0_test=which(test_y==0)
        index1_test=which(test_y==1)

        for(j in 1:num_of_model){
          score = p[k,,j]
          roc <- roc.curve(scores.class0 =score[index1_test],scores.class1=score[index0_test],curve =FALSE)

          auc[count,k,j]=roc$auc
        }

      }
      count = count + 1
    }
    if(true_data_to_compare==0){break;}
  }

  return(auc)
}
