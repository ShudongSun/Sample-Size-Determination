#' @importFrom randomForest randomForest
#' @importFrom e1071 svm
#' @importFrom xgboost xgboost
#' @importFrom glmnet cv.glmnet
#' @importFrom MASS lda
#' @importFrom naivebayes naive_bayes
#' @importFrom ada ada
#' @importFrom tree tree
#' @importFrom smotefamily SMOTE
#' @title smote
#' @description  Use SMOTE to generate data and train different models.
#' @param x_pilot input variables of pilot data
#' @param y_pilot labels of pilot data
#' @param n0_train the number of training data of class 0
#' @param n1_train the number of training data of class 1
#' @param n0_test the number of test data of class 0
#' @param n1_test the number of test data of class 1
#' @param method base classification method.
#' \itemize{
#' \item logistic: Logistic regression. \link{glm} function with family = 'binomial'
#' \item penlog: Penalized logistic regression with LASSO penalty. \code{\link[glmnet]{glmnet}} in \code{glmnet} package
#' \item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
#' \item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
#' \item lda: Linear Discriminant Analysis. \code{\link[MASS]{lda}} in \code{MASS} package
#' \item slda: Sparse Linear Discriminant Analysis with LASSO penalty.
#' \item nb: Naive Bayes. \code{\link[e1071]{naiveBayes}} in \code{e1071} package
#' \item nnb: Nonparametric Naive Bayes. \code{\link[naivebayes]{naive_bayes}} in \code{naivebayes} package
#' \item ada: Ada-Boost. \code{\link[ada]{ada}} in \code{ada} package
#' \item xgboost: XGBboost. \code{\link[xgboost]{xgboost}} in \code{xgboost} package
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' \item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
#' }
#' @param func If you set "method" to "self", you have to pass your self-defined model function. This function should be able to take "x_train" and "y_train" as the first two inputs to train the model and then take "x_test" as the third input and return the predicted scores of x_test data. For example, \cr\cr
#' \code{library(e1071)\cr\cr
#' predict_model <- function(x_train, y_train, x_test){ \cr
#' data_trainxy<-data.frame(x_train,y_train=as.factor(y_train)) \cr
#' fit_svm<-svm(y_train~.,data=data_trainxy,probability=TRUE) \cr
#' pred_svm <- predict(fit_svm, x_test, probability=TRUE,decision.values = TRUE) \cr
#' p_svm=as.data.frame(attr(pred_svm, "probabilities"))$"1" \cr
#' return(p_svm) \cr
#' }\cr \cr
#' result = pilot_tfe_smote(x_pilot,y_pilot,n0_train,n1_train,n0_test,n1_test,method=c("self","randomforest"),func=predict_model)}
#'
#' @return the scores predicted by models
#' @export
#'
#' @examples
#' library(mvtnorm)
#' library(MASS)
#'
#' df = 10
#' rho=0.5
#' d=5
#' delta = rep(2,d)
#' H<-abs(outer(1:d,1:d,"-"))
#' covxx=rho^H
#'
#' n1_all <- n0_all <- 800
#' n1_p <- n0_p <- 15
#'
#'x0_all = rmvt(n = n0_all, sigma = covxx, delta = rep(0,d), df = df)
#'x1_all = rmvt(n = n1_all, sigma = covxx, delta = delta, df = df)
#'
#'x_data = rbind(x0_all,x1_all)
#'y_data = c(rep(0,n0_all),rep(1,n1_all))
#'
#'id0 <- which(y_data==0)
#'id1 <- which(y_data==1)
#'
#'id0_p <- sample(id0,n0_p)
#'id1_p <- sample(id1,n1_p)
#'id_p <- c(id0_p,id1_p)
#'
#'x_pilot <- as.matrix(x_data[id_p,])
#'y_pilot <- as.matrix(y_data[id_p])
#'
#'n1_train <- n0_train <- n_train <-60
#'n0_test <- n1_test <- 300
#'
#'result = pilot_tfe_smote(x_pilot,y_pilot,n0_train,n1_train,n0_test,n1_test,method=c("svm","randomforest"))
#'
pilot_tfe_smote <- function(x_pilot,y_pilot,n0_train,n1_train,n0_test,n1_test,method=c("svm","randomforest"),func=NULL)
{
  library(MASS)
  library(rlist)
  # library(smotefamily)

  # library(DMwR)

  id0_p = which(y_pilot==0)
  id1_p = which(y_pilot==1)

  n0_p = length(id0_p)
  n1_p = length(id1_p)

  n0_tt = n0_train + n0_test
  n1_tt = n1_train + n1_test

  x_pilot_df = data.frame(x_pilot)
  y_pilot_df = data.frame(y_pilot)
  pilot_df = cbind(x_pilot_df,y_pilot_df)
  # pilot_df$y_pilot = as.factor(pilot_df$y_pilot)
  table(pilot_df$y_pilot)

  n_min_p = min(n0_p,n1_p)
  # n_min_tt = min(n0_tt,n1_tt)
  n_max_tt = max(n0_tt,n1_tt)

  # a = ceiling(n_min_tt/n_min_p-1)*100
  # b = ceiling(1e4*n_max_tt/a/n_min_p)
  label_col_num = grep("y_pilot",colnames(pilot_df))

  tempData1 <- SMOTE(pilot_df[,-label_col_num], pilot_df[,label_col_num], K=5, dup_size = ceiling(n_max_tt/n_min_p))
  tempData2 <- SMOTE(tempData1[["data"]][,-label_col_num], tempData1[["data"]][,label_col_num], K=5, dup_size = ceiling(n_max_tt/n_min_p))
  # newData <- SMOTE(y_pilot ~ ., pilot_df, perc.over = a, perc.under=b)
  newData = tempData2[["data"]]

  table(newData$class)

  id0 <- which(newData["class"]==0)
  id1 <- which(newData["class"]==1)

  ###train
  id0_train <- sample(id0,n0_train)
  id1_train <- sample(id1,n1_train)
  id_train <- c(id0_train,id1_train)

  x_train <- as.matrix(newData[,-grep("class",colnames(newData))][id_train,])
  y_train <- as.matrix(newData[,"class"][id_train])

  train_x=as.data.frame(x_train)
  train_y=y_train
  data_trainxy<-data.frame(train_x,y_train)

  ###test
  id0_remain = setdiff(id0,id0_train)
  id1_remain = setdiff(id1,id1_train)

  id0_test <- sample(id0_remain,n0_test)
  id1_test <- sample(id1_remain,n1_test)
  id_test <- c(id0_test,id1_test)

  x_test <- as.matrix(newData[,-grep("class",colnames(newData))][id_test,])
  y_test <- as.matrix(newData[,"class"][id_test])

  test_x=as.data.frame(x_test)
  # test_y=as.data.frame(y_test)



  # ###train
  # train_x0=rMvdc(n0_train, mymvd0)
  # train_x1=rMvdc(n1_train, mymvd1)
  # trx=matrix(rbind(train_x0,train_x1),n_train,num_feature)
  # train_x=as.data.frame(trx)
  #
  # ###test
  # test_x0=rMvdc(n0_test, mymvd0)
  # test_x1=rMvdc(n1_test, mymvd1)
  # tex=matrix(rbind(test_x0,test_x1),n_test,num_feature)
  # test_x=as.data.frame(tex)
  #
  # data_trainxy<-data.frame(train_x,train_y)

  methods_all = c("logistic", "penlog", "svm", "randomforest", "lda", "slda", "nb", "nnb", "ada", "tree","xgboost","self")

  p_results = numeric()
  # print(method)
  for (i in 1:length(method)){
    # print(method[i])
    if(!method[i] %in% methods_all){
      stop('method \'',method[i], '\' cannot be found')
    }

    ###LR
    if(method[i] == "logistic"){
      fit_LR<-suppressWarnings(glm(train_y~.,family = "binomial",data=data_trainxy, maxit=100))
      prep_LR<-predict(fit_LR,test_x)
      p_LR<-1/(1+exp(-prep_LR))
      p_results=rbind(p_results,p_LR)
    }

    ###xgboost
    if(method[i] == "xgboost"){
      new_trainx<-as.matrix(train_x)
      dx_trainy<-xgb.DMatrix(data = new_trainx, label = train_y)
      fit_xgb <- xgboost(data=dx_trainy, nthread=3,nrounds=100,objective = "binary:logistic", verbose = 0)
      p_xgb<-predict(fit_xgb,as.matrix(test_x))
      p_results=rbind(p_results,p_xgb)
    }

    ###SVM
    if(method[i] == "svm"){
      data_trainxy<-data.frame(train_x,train_y=as.factor(train_y))
      fit_svm<-svm(train_y~.,data=data_trainxy,probability=TRUE)
      pred_svm <- predict(fit_svm, test_x, probability=TRUE,decision.values = TRUE)
      p_svm=as.data.frame(attr(pred_svm, "probabilities"))$"1"
      p_results=rbind(p_results,p_svm)
    }

    ###RF
    if(method[i] == "randomforest"){
      data_trainxy<-data.frame(train_x,train_y=as.factor(train_y))
      fit_RF<-randomForest(train_y~.,data = data_trainxy,importance=TRUE)
      p_RF=predict(fit_RF,test_x,type = "prob")[, 2]
      p_results=rbind(p_results,p_RF)
    }

    ###Penalized logistic regression with LASSO penalty
    if(method[i] == "penlog"){
      fit_penlog = cv.glmnet(as.matrix(train_x), train_y, family = "binomial")
      p_penlog = t(predict(fit_penlog$glmnet.fit, newx = as.matrix(test_x), type = "response", s = fit_penlog$lambda.min))
      rownames(p_penlog)="p_penlog"
      p_results=rbind(p_results,p_penlog)
    }

    ###Linear Discriminant Analysis
    if(method[i] == "lda"){
      fit_lda = lda(as.matrix(train_x), train_y)
      p_lda = predict(fit_lda, as.matrix(test_x))$posterior[, 2]
      p_results=rbind(p_results,p_lda)
    }

    ###Sparse Linear Discriminant Analysis with LASSO penalty
    if(method[i] == "slda"){
      n1 = sum(train_y==1)
      n0 = sum(train_y==0)
      n = n1 + n0
      y_lda = train_y
      y_lda[train_y == 0] = -n/n0
      y_lda[train_y == 1] = n/n1
      fit_slda = cv.glmnet(as.matrix(train_x), y_lda)
      score_slda = t(predict(fit_slda$glmnet.fit, newx = as.matrix(test_x), type = "link", s = fit_slda$lambda.min))
      p_slda = 1/(1+exp(-score_slda))
      rownames(p_slda)="p_slda"
      p_results=rbind(p_results,p_slda)
    }

    ###Naive Bayes
    if(method[i] == "nb"){
      train_data_nb = data.frame(train_x, y = train_y)
      fit_nb <- naive_bayes(as.factor(y) ~ ., data = train_data_nb, usekernel = FALSE)
      p_nb = predict(fit_nb, data.frame(test_x), type = "prob")[,2]
      p_results=rbind(p_results,p_nb)
    }

    ###Nonparametric Naive Bayes
    if(method[i] == "nnb"){
      train_data_nnb = data.frame(train_x, y = train_y)
      fit_nnb <- naive_bayes(as.factor(y) ~ ., data = train_data_nnb, usekernel = TRUE)
      p_nnb = predict(fit_nnb, data.frame(test_x), type = "prob")[,2]
      p_results=rbind(p_results,p_nnb)
    }

    ###Ada-Boost
    if(method[i] == "ada"){
      train_data_ada = data.frame(train_x, y = train_y)
      fit_ada = ada(y ~ ., data = train_data_ada)
      p_ada = predict(fit_ada, data.frame(test_x), type = "probs")[, 2]
      p_results=rbind(p_results,p_ada)
    }

    ###Classification
    if(method[i] == "tree"){
      # train_y = as.factor(train_y)
      train_data_tree = data.frame(train_x, y = train_y)
      fit_tree = tree(y~ ., data = train_data_tree)
      p_tree = predict(fit_tree, newdata = data.frame(test_x), type = 'vector')
      p_results=rbind(p_results,p_tree)
    }

    ###Self-defined
    if(method[i] == "self"){
      p_self = func(train_x, train_y, test_x)
      p_results=rbind(p_results,p_self)
    }

  }

  return(p_results)

}
